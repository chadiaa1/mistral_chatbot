# app.py - Version avec gestion optimis√©e des timeouts
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import requests
import json
from datetime import datetime
import logging
import os
import time
from typing import Dict, List, Optional
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
import threading

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

class CBTChatbot:
    def __init__(self):
        self.conversation_history: List[Dict] = []
        self.max_history_length = 5  # R√©duit pour des prompts plus courts
        self.ollama_url = os.getenv('OLLAMA_URL', 'http://localhost:11434')
        self.model_name = os.getenv('MODEL_NAME', 'mistral_cbt')
        
        # R√©ponses de fallback pour les cas de timeout
        self.fallback_responses = [
            "Je comprends que tu traverses une p√©riode difficile. Peux-tu me parler de ce que tu ressens en ce moment ?",
            "C'est important d'explorer ces sentiments. Qu'est-ce qui se passe dans ta t√™te quand tu vis cette situation ?",
            "Je t'entends. Prenons un moment pour identifier les pens√©es qui accompagnent ces √©motions.",
            "Ces √©motions sont valides. Comment pourrions-nous examiner les pens√©es qui les alimentent ?",
            "Merci de partager cela avec moi. Qu'est-ce qui te vient √† l'esprit quand tu penses √† cette situation ?"
        ]
        
        # Techniques CBT simplifi√©es pour r√©ponses rapides
        self.quick_cbt_techniques = {
            "stress": "Quand tu te sens stress√©, quelles pens√©es traversent ton esprit ? ü§î",
            "anxi√©t√©": "L'anxi√©t√© peut amplifier nos craintes. Quelles preuves as-tu que tes peurs vont se r√©aliser ?",
            "tristesse": "La tristesse est une √©motion importante. Qu'est-ce qu'elle essaie de te dire ?",
            "col√®re": "La col√®re peut masquer d'autres √©motions. Qu'y a-t-il sous cette col√®re ?",
            "culpabilit√©": "Tu sembles te bl√¢mer. Et si on examinait tous les facteurs en jeu ?",
            "perfectionnisme": "Le perfectionnisme peut √™tre √©puisant. Qu'est-ce qui serait 'assez bien' ?",
            "catastrophe": "Tu imagines le pire sc√©nario. Quelles autres issues sont possibles ?",
            "noir_blanc": "Tu vois les choses en tout ou rien. O√π pourrait se situer le juste milieu ?"
        }
    
    def check_ollama_status(self) -> bool:
        """V√©rifie si Ollama est accessible"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def detect_keywords(self, message: str) -> str:
        """D√©tection simple et rapide de mots-cl√©s √©motionnels"""
        message_lower = message.lower()
        
        keyword_map = {
            "stress": ["stress√©", "stress", "pression", "tendu"],
            "anxi√©t√©": ["anxieux", "angoisse", "inquiet", "nerveux"],
            "tristesse": ["triste", "d√©prim√©", "m√©lancolique", "abattu"],
            "col√®re": ["col√®re", "√©nerv√©", "irrit√©", "furieux"],
            "culpabilit√©": ["coupable", "faute", "responsable", "bl√¢me"],
            "perfectionnisme": ["parfait", "parfaitement", "impeccable"],
            "catastrophe": ["catastrophe", "terrible", "horrible", "affreux"],
            "noir_blanc": ["toujours", "jamais", "tout", "rien"]
        }
        
        for emotion, keywords in keyword_map.items():
            if any(keyword in message_lower for keyword in keywords):
                return emotion
        
        return "general"
    
    def generate_quick_response(self, user_message: str, detected_emotion: str) -> str:
        """G√©n√®re une r√©ponse rapide bas√©e sur les mots-cl√©s d√©tect√©s"""
        
        if detected_emotion in self.quick_cbt_techniques:
            base_response = self.quick_cbt_techniques[detected_emotion]
        else:
            # Utiliser une r√©ponse de fallback al√©atoire bas√©e sur la longueur du message
            import hashlib
            hash_obj = hashlib.md5(user_message.encode())
            index = int(hash_obj.hexdigest(), 16) % len(self.fallback_responses)
            base_response = self.fallback_responses[index]
        
        # Ajouter une question de suivi contextuelle
        follow_up_questions = [
            "Peux-tu me d√©crire cette situation plus pr√©cis√©ment ?",
            "Qu'est-ce qui rend cette situation particuli√®rement difficile pour toi ?",
            "Comment ton corps r√©agit-il dans ces moments ?",
            "Quelles pens√©es reviennent le plus souvent ?",
            "Y a-t-il un √©l√©ment d√©clencheur que tu as remarqu√© ?"
        ]
        
        # Choisir une question de suivi bas√©e sur l'√©motion
        question_index = hash(detected_emotion) % len(follow_up_questions)
        follow_up = follow_up_questions[question_index]
        
        return f"{base_response}\n\n{follow_up}"
    
    def call_ollama_with_timeout(self, prompt: str, timeout: int = 15) -> Optional[str]:
        """Appel √† Ollama avec timeout strict et fallback"""
        
        def make_request():
            try:
                response = requests.post(
                    f'{self.ollama_url}/api/generate',
                    json={
                        'model': self.model_name,
                        'prompt': prompt,
                        'stream': False,
                        'options': {
                            'temperature': 0.7,
                            'top_p': 0.9,
                            'num_predict': 150,  # R√©duit pour des r√©ponses plus rapides
                            'stop': ['[INST]', '</s>', '\n\n\n']
                        }
                    },
                    timeout=timeout
                )
                
                if response.status_code == 200:
                    result = response.json()['response'].strip()
                    # Nettoyer la r√©ponse
                    if result.startswith('</s>'):
                        result = result[4:].strip()
                    return result
                else:
                    logger.error(f"Erreur Ollama HTTP {response.status_code}")
                    return None
                    
            except Exception as e:
                logger.error(f"Erreur lors de l'appel Ollama: {e}")
                return None
        
        # Utiliser ThreadPoolExecutor pour un timeout strict
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(make_request)
            try:
                return future.result(timeout=timeout)
            except FutureTimeoutError:
                logger.warning(f"Timeout Ollama apr√®s {timeout}s")
                return None
            except Exception as e:
                logger.error(f"Erreur inattendue: {e}")
                return None
    
    def generate_response(self, user_message: str) -> str:
        """G√©n√®re une r√©ponse avec strat√©gie de fallback intelligente"""
        
        # D√©tection rapide d'√©motion/th√®me
        detected_emotion = self.detect_keywords(user_message)
        
        # V√©rifier l'√©tat d'Ollama avant d'essayer
        if not self.check_ollama_status():
            logger.warning("Ollama n'est pas accessible, utilisation du fallback")
            response = self.generate_quick_response(user_message, detected_emotion)
        else:
            # Construire un prompt optimis√© et court
            context = ""
            if self.conversation_history:
                # Prendre seulement le dernier √©change pour r√©duire la taille
                last_exchange = self.conversation_history[-1]
                context = f"Contexte: L'utilisateur a dit '{last_exchange['user']}' et tu as r√©pondu '{last_exchange['bot'][:100]}...'\n"
            
            optimized_prompt = f"""<s>[INST] Tu es un th√©rapeute CBT. {context}
L'utilisateur dit: "{user_message}"

R√©ponds bri√®vement (2-3 phrases max) en fran√ßais avec empathie et une question th√©rapeutique. [/INST]"""
            
            # Essayer d'appeler Ollama avec timeout court
            start_time = time.time()
            ollama_response = self.call_ollama_with_timeout(optimized_prompt, timeout=15)
            call_duration = time.time() - start_time
            
            if ollama_response:
                logger.info(f"R√©ponse Ollama obtenue en {call_duration:.2f}s")
                response = ollama_response
                
                # Ajouter une technique CBT si la r√©ponse est courte
                if len(response) < 100 and detected_emotion in self.quick_cbt_techniques:
                    response += f"\n\nüí° {self.quick_cbt_techniques[detected_emotion]}"
            else:
                logger.warning(f"Fallback apr√®s √©chec Ollama (dur√©e: {call_duration:.2f}s)")
                response = self.generate_quick_response(user_message, detected_emotion)
        
        # Sauvegarder dans l'historique
        self.conversation_history.append({
            'user': user_message,
            'bot': response,
            'timestamp': datetime.now().isoformat(),
            'detected_emotion': detected_emotion,
            'source': 'ollama' if 'ollama_response' in locals() and ollama_response else 'fallback'
        })
        
        # Limiter l'historique
        if len(self.conversation_history) > self.max_history_length:
            self.conversation_history = self.conversation_history[-self.max_history_length:]
        
        return response

# Instance globale
chatbot = CBTChatbot()

@app.route('/')
def index():
    return render_template('chat.html')

@app.route('/chat', methods=['POST'])
def chat():
    try:
        data = request.json
        user_message = data.get('message', '').strip()
        
        if not user_message:
            return jsonify({'error': 'Message vide'}), 400
        
        if len(user_message) > 500:  # Limite plus stricte
            return jsonify({'error': 'Message trop long (500 caract√®res max)'}), 400
        
        # Mesurer le temps de r√©ponse
        start_time = time.time()
        response = chatbot.generate_response(user_message)
        response_time = time.time() - start_time
        
        logger.info(f"R√©ponse g√©n√©r√©e en {response_time:.2f}s")
        
        return jsonify({
            'response': response,
            'timestamp': datetime.now().isoformat(),
            'response_time': round(response_time, 2)
        })
        
    except Exception as e:
        logger.error(f"Erreur dans /chat: {e}")
        return jsonify({'error': 'Erreur interne du serveur'}), 500

@app.route('/reset', methods=['POST'])
def reset_conversation():
    try:
        chatbot.conversation_history = []
        return jsonify({'message': 'Conversation r√©initialis√©e'})
    except Exception as e:
        logger.error(f"Erreur lors de la r√©initialisation: {e}")
        return jsonify({'error': 'Erreur lors de la r√©initialisation'}), 500

@app.route('/status', methods=['GET'])
def get_status():
    """Endpoint pour v√©rifier l'√©tat du syst√®me"""
    ollama_status = chatbot.check_ollama_status()
    return jsonify({
        'ollama_connected': ollama_status,
        'model': chatbot.model_name,
        'conversation_length': len(chatbot.conversation_history),
        'last_activity': chatbot.conversation_history[-1]['timestamp'] if chatbot.conversation_history else None
    })

if __name__ == '__main__':
    # V√©rifier Ollama au d√©marrage
    if chatbot.check_ollama_status():
        logger.info("‚úÖ Ollama est accessible")
    else:
        logger.warning("‚ö†Ô∏è  Ollama n'est pas accessible - mode fallback activ√©")
    
    port = int(os.environ.get('PORT', 5000))
    debug_mode = os.environ.get('DEBUG', 'True').lower() == 'true'
    
    app.run(debug=debug_mode, host='0.0.0.0', port=port)